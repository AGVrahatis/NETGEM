%%% supplement.tex --- 
%% 
%% Filename: supplement.tex
%% Description: 
%% Author: Vinay Jethava
%% Maintainer: 
%% Created: Sat Jan  9 14:32:47 2010 (+0530)
%% Version: 
%% Last-Updated: Thu Jan 21 20:15:46 2010 (+0530)
%%           By: Vinay Jethava
%%     Update #: 19
%% URL: 
%% Keywords: 
%% Compatibility: 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Commentary: 
%% 
%% 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Change log:
%% 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%% This program is free software; you can redistribute it and/or
%% modify it under the terms of the GNU General Public License as
%% published by the Free Software Foundation; either version 3, or
%% (at your option) any later version.
%% 
%% This program is distributed in the hope that it will be useful,
%% but WITHOUT ANY WARRANTY; without even the implied warranty of
%% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
%% General Public License for more details.
%% 
%% You should have received a copy of the GNU General Public License
%% along with this program; see the file COPYING.  If not, write to
%% the Free Software Foundation, Inc., 51 Franklin Street, Fifth
%% Floor, Boston, MA 02110-1301, USA.
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Code:

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{hyperref}
% \usepackage{psfrags}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\newcommand{\todo}[1]{\textcolor{red}{#1}}

\title{Supplementary material for manuscript:  ``NETGEM: Network Embedded analysis of Temporal Gene Expression Model''}
\author{Vinay Jethava}
\begin{document}
\maketitle

\setcounter{tocdepth}{3}
%\tableofcontents
\vspace*{1cm}

This document presents the supplemental material for the manuscript
``NETGEM: Network Embedded analysis of Temporal Gene Expression
Model''.  

\subsection{Original Model Description}

\subsubsection{Observation model}
\label{observation-model}

The observed gene expression levels, $\mathbf{x}^{s}(t)$, for an strain
$s$ at time $t$ are modeled as an Ising system \cite{Song09KELLER}:
\begin{equation}
\label{eq:ising}
 P\left(\mathbf{x}^{s}(t) | \mathbf{w}^{s}(t)\right) = 
      \frac{1}{Z(t)} \exp \left( - \sum_{(i,j) \in E} w^s_{(i,j)}(t)
        x^{s}_i(t) x^{s}_j(t)\right)  
\end{equation}
where $Z(t)$ is the normalization constant. 

\subsubsection{Evolution model }
\label{sec:evolution-model}
We assume that the weights evolve according to the Markov chain, i.e.,
\begin{equation}
  \label{eq:q-evol}
 P(\mathbf{w}(t+1) = \mathbf{w}_{t+1} |  \mathbf{w}(t) = \mathbf{w}_{t}) = \mathbf{Q}(\mathbf{w}_{t}, \mathbf{w}_{t+1})
\end{equation}
where $\mathbf{Q}(\mathbf{w}_{t}, \mathbf{w}_{t+1})$ is the probability of the
transition from state $\mathbf{w}_{t}$ at time $t$ to state
$\mathbf{w}_{t+1}$ at time $(t+1)$. In general, if there are $S$
strains present, then each will have corresponding transition
probability matrix $\mathbf{Q}_{s}$. 


\section{ NETGEM model derivation}
\label{sec:mixt-model-deriv}
This section presents the derivation for the inference and parameter
learning in the generative model, NETGEM. The inference is done over
the hidden variables $\{y_{e}(t), w_{e}(t)\}$ based on the Markovian structure of the problem.

 We assume that the data
for strain, $s$, is independently generated 
based on the Ising model with the weights that are
damped versions of the weights in the original 
strain. This leads to the observation model specified as:
\begin{eqnarray}
  \label{eq:obs} o^{t}_{e}(l) &=& P(x_{e}^{1:S}(t) | w_{e}(t) = w_{l}) \\
\label{eq:obs2} & = & \frac{1}{Z} \prod_{s=1}^{S} P(x_{e}^{s}(t) |
w_{e}(t) = w_{l}) \\ 
\label{eq:obs3} & = & \frac{\exp\left\{ - w_{l}\big(\sum_{s=1}^{S}
      x^{s}_{i}(t)x^{s}_{j}(t) \Gamma^{s}(i,j)\big)
  \right\}}{\sum^{\mathcal W}_{l=1}\exp\left\{ - w_{l}\big(\sum_{s=1}^{S}  x^{s}_{i}(t)x^{s}_{j}(t) \Gamma^{s}(i,j)\big) \right\}}
\end{eqnarray}

%The forward iterates, $f_e^{t}(m)$, and the backward iterates,
%$b^t_e(m)$, remain unchanged as in (\ref{eq:update})-(\ref{eq:update-1}). 
The forward iterates, $f_e^t(l, h)$ and backward iterates, $b_e^t(l,
h)$ can  be computed as follows: 
\begin{eqnarray}
  \label{eq:mixture-update}
  f^{t}_{e}(m, h) &=& P(x_{e}^{1:S}(1:t) , w_{e}^{t} = w_{m},
  y_e^t = h | \Psi^{(n)}_{e}) \\
&=& P(x^{1:S}_e(t) | w_m) \sum_{w_l} \sum_{h'}
\Big[ P(y^t_e=h | \alpha^{(n)})  \nonumber \\
&\times& P(w_m | w^{t-1}_e = w_l, y^{t-1}_e = h')  \times f_e^{t-1}(l, h')
\Big]\\
%% final forward equation
&=& o^{t}_{e}(m) \sum_{l=1}^{{\mathcal W}} \sum_{h'=1}^H
f_{e}^{t-1}(l, h') \alpha_h^{(n)} q_{h'}^{(n)}(l, m) \\
%% backward equations
  b^{t}_{e}(m, h) &=& P(x^{1:S}_{e}((t+1):T) | w_{e}(t) = w_{m}, y_e^{t}=h,
  \Psi^{(n)}_{e})  \\
&=&  \sum_{w_l} \sum_{h'} \Big[ P(x^{1:S}_e(t+1) | w^{t+1}_e = w_l)
  b_e^{t+1}(l, h')\nonumber \\
&\times& P(w^{t+1}_e = w_l | w_m, y^{t}_e = h)   P(y^{t+1}_e=h' | \alpha^{(n)}) 
\Big]\\
%% final backward equation
\label{eq:mixture-update-1}
&=& \sum_{m=1}^{{\mathcal W}}\sum_{h'=1}^H q_{h}^{(n)}(m, l) o^{t+1}_{e}(l) \alpha^{(n)}_{h'}b^{t+1}_{e}(l,h')
\end{eqnarray}
The conditional probability $P(\Omega_e^{t}=(w_l, h), \Omega_e^{t+1}
=(w_m, h') | \mathbf{x}_e^{1:S}(1:T) , \Psi^{(n)})$ denoted by $\xi^t_e(l,m,h,h') $ can be computed as
\begin{equation}
  \label{eq:mixture-p-cond}
  \xi^t_e(l,m,h,h') \propto f^t_e(l,h)\alpha_{h'}^{(n)} q^{(n)}_h(l,m)
  o^{t+1}_e(m) b_e^{t+1}(m, h')
\end{equation}
The likelihood term, $ {\mathcal L}(\Psi; \Psi^{(n)})$, in
(\ref{eq:q-edge-mixture-em}) can be expressed 
in terms of the conditioned edge probabilities, $\xi^t_e$, in
(\ref{eq:mixture-p-cond}) as 
\begin{eqnarray}
  \label{eq:mixture-L-edge}
  {\mathcal L}(\Psi; \Psi^{(n)}) &=& \sum_{e\in E}\sum_{t=1}^{T-1}
  \mathbf{E}_{\xi^t_e}[\ln q_h(l, m) + \ln \alpha_{e,h'}]
\end{eqnarray}
subject to the constraints
\begin{eqnarray}
  \label{eq:mixture-q-constraint} \sum_{m} q_h(l,m) &=& 1 \quad\forall h \\
   \label{eq:mixture-alpha-constraint} \sum_{h} \alpha_{e,h} &=& 1 \quad\forall e
\end{eqnarray}

\section{Discussion on NETGEM vs Naive HMM}
\label{sec:discussion}
This section presents a study of the naive HMM vs the NETGEM model. 

One can model the dynamics of the simple
model defined by (\ref{eq:ising}) and (\ref{eq:q-evol}), using a
simple HMM. The quantity to be estimated is the transition probability
$\mathbf Q$. However, the exponential state space makes such an
approach impractical. 

For the purpose of study, we explore the relation between our model
and the simple HMM in the following two sections. We also present a comparison of
the results obtained using the independent weights evolution
assumption vs the results using a standard HMM implementation in  section~\ref{sec:validation}.

\subsubsection{Evaluation of independent weights dynamics} 
\label{sec:validity-assumption}
We now discuss the
relationship between the results obtained using the independent
weights evolution assumption and the original problem.  
\begin{lemma}
The independently evolving weights assumption gives a rank-1 tensor approximation~\cite{Horn90}  to the global weight
transition probability.  
\end{lemma}
For example, if the interaction network consists of two edges having transition
probabilities $A = \{a_{ij}\}$
 %$Q_{a}= \left\{\begin{array}{cc}   a_{11} & a_{12} \\     a_{21} &
 %a_{22} \end{array}\right\}$ 
and $B$ respectively, the approximation to
the original matrix, $\hat{Q}$ is given as the Kronecker product
\[ \hat{Q} = A \otimes B = \left[
  \begin{array}{cc}
    a_{11} B & a_{12} B \\
    a_{21} B & a_{22} B
  \end{array}
\right]
\]

We note that if the weights indeed evolve independently of each other
and we are given the edge transition probabilities
$Q_{e}$, the overall system transition probability, $\mathbf{Q}$, is
given as 
\[
\mathbf{Q} = Q_{1} \otimes Q_{2} \otimes \ldots \otimes Q_{E}
\]
In general, if there are $E$ edges with estimated transition
probability matrices, $\hat{Q}_{1}, \ldots, \hat{Q}_{E}$, we obtain the approximation, $\hat{\mathbf{Q}}$
\[
\hat{\mathbf{Q}} \simeq Q_{1} \otimes Q_{2} \otimes \ldots \otimes Q_{E}
\]
We discuss the quality of approximation in case the original
probability matrix $Q$ is a higher order tensor
elsewhere. 

\subsubsection{Number of parameters to be learnt}
\label{sec:parameter-reduction}
We note that the generative model, NETGEM, does an order reduction the number of
parameters to be learnt. Specifically, if there are  $E$  edges in the
network, with each edge taking on  $W$ denotes the
number of possible interaction strengths, and $H$ is the number of
functional classesfrom $O(W^{2*E})$ to $O(H W^{2} + H E )$,
where . Typical values are $E \sim 6000$, $H \sim 260$
and $W\sim 5$.

\section{Independent weights dynamics}
\label{sec:fact-model-deriv}
This section presents a derivation of the forward and backward
probabilities for the independent weights evolution model in the
presence of multiple strains. The observation model,
$o^t_e(l)=P(x^{1:S}_e(t) | w^t_e=w_l)$ , remains unchanged as in (\ref{eq:obs})-(\ref{eq:obs3}). 

The update equations for computing the forward and backward
probability distributions are given as:
\begin{eqnarray}
  \label{eq:update}
  f^{t+1}_{e}(m) &=& P(x_{e}^{1:S}(1:t) , w_{e}(t+1) = w_{m} | Q^{(n)}_{e}) \\
&=& o^{t+1}_{e}(m) \sum_{l=1}^{{\mathcal W}} f_{e}^{t}(l) q_{e}^{(n)}(l, m) \\
  b^{t}_{e}(l) &=& P(x^{1:S}_{e}((t+1):T) | w_{e}(t) = w_{l},
  Q^{(n)}_{e})  \\
\label{eq:update-1}
&=& \sum_{m=1}^{{\mathcal W}} q_{e}^{(n)}(l, m) o^{t+1}_{e}(m) b^{t+1}_{e}(m)
\end{eqnarray}
and the joint probability as
\begin{eqnarray}
  \label{eq:p-joint}
  \xi_{e}^{t}(l,m) &=& P(w_{e}(t,t+1) =(w_{l}, w_{m}) |
  x^{1:S}_{e}(1:T), Q^{(n)}_{e}) \\
&\propto& f_{e}^{t}(l) q_{e}^{(n)}(l, m) o_{e}^{t+1}(m) b^{t+1}(m) 
\end{eqnarray}

We use the independent weights dynamics where the parameter to be learnt is
the transition matrix $Q_{e}$ for each edge, $e = (i, j) \in E$.  We solve the Expectation
Maximization~(EM)\cite{Dempster77em} problem for each edge,
$e$,
\begin{equation}
  \label{eq:q-edge_em_map}
  \begin{array}[h]{llll}
  \textrm{E-step:} & {\mathcal L}(Q_{e}; Q_{e}^{(n)}) &=&
  E_{w_{e}^{|} }  [ \ln P(\mathbf{x}_{e}^{1:S}(1:T),
  \mathbf{w}_{e}({1:T}) | Q_{e})] \\
\vspace{2 mm}
  \textrm{M-step:} & \hat{Q}_{e}^{(n+1)} &=& \arg\max_{Q_{e}} (\ln P(Q_{e}) +
  {\mathcal L}(Q_{e}; Q_{e}^{(n)} ) ) 
  \end{array}
\end{equation}
where $W^{|}_{e}$ is the conditioned variable, $w_{e}(1:T)|
\mathbf{x}_{e}^{1:S}(1:T), Q_{e}^{(n)}$ and $Q_{e}^{(n)}$ is the
MAP estimate for the transition probability, $Q_{e}$, at the $n^{th}$
iteration of the algorithm. 

This leads to the update equation for the MAP estimate for transition
probabilities, $q(l,m)$, obtained by the maximization step in (\ref{eq:q-edge_em_map}) as
\begin{equation}
  \label{eq:q-update}
  q^{(n+1)}_{e}(l, m) = \frac{(\theta_{lm}-1) + \sum_{t=1}^{T-1} \xi^{t}_{e}(l,
    m) }{\sum_{m} (\theta_{lm} -1) + \sum^{T-1}_{t=1} \sum^{\mathcal
      W}_{m=1} \xi^{t}_{e}(l,m)}
\end{equation}
% where $\xi^{t}_{e}(l,m)$ is defined in the appendix~\ref{sec:fact-model-deriv}.

\bibliographystyle{bioinformatics}
\bibliography{default}
\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% supplement.tex ends here
